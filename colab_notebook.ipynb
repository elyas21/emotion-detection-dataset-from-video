{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Emotion detection model from classification model"
      ],
      "metadata": {
        "id": "DTNzpHTjhh1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Load and prepare dataset\n"
      ],
      "metadata": {
        "id": "jD3Q2MMe21tG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load video "
      ],
      "metadata": {
        "id": "JCLBz8WKhdQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4J_STkMvejm",
        "outputId": "49efde62-af55-4a24-e4b9-dbcac5b349ed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFrxvrpOwGCV",
        "outputId": "c9f784f2-7f9a-44ca-aab8-e8f8352bfcea"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install opencv-contrib-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFP7Kw2rynpy",
        "outputId": "ae616d3c-db3e-49e1-a92d-5dcc5c3f99f9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from opencv-contrib-python) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dirctories"
      ],
      "metadata": {
        "id": "IvReNYQ40vYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil , os\n",
        "\n",
        "if os.path.exists('data/rowImg'):\n",
        "  shutil.rmtree('data/rowImg')"
      ],
      "metadata": {
        "id": "iyLsSzneFRnK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftu9y0PzH1Te"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirs = [\n",
        "    'data',\n",
        "    'data/rowVid',\n",
        "    'data/classification',\n",
        "    'data/rowImg']\n",
        "\n",
        "[os.makedirs(p) if not os.path.exists(p) else ''  for p in dirs]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEcV7s4_yGZM",
        "outputId": "8337b957-d839-4af2-87e3-0092b5cd5da0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cp -r /gdrive/MyDrive/DataEnginnering/* /content/data/rowVid"
      ],
      "metadata": {
        "id": "MM1ZDvs8xGBI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import os\n",
        "trial, count = 10, 0\n",
        "\n",
        "for video in os.listdir('data/rowVid'):\n",
        "  # if trial>count:\n",
        "    if video.endswith('mp4') or video.endswith('avi'):\n",
        "      cap = cv.VideoCapture('/content/data/rowVid/'+video)\n",
        "      c=1\n",
        "      success,image = cap.read()\n",
        "      while success:\n",
        "        if c%100==0:\n",
        "          # resize image into input shape\n",
        "          # to be implement\n",
        "          cv.imwrite(f'/content/data/rowImg/'+ str(c) + '-' + video[:-4] + '.png', image)\n",
        "          success,image = cap.read()\n",
        "          print('Read a new frame: ', success)\n",
        "        \n",
        "        c= c+1\n",
        "    \n",
        "      cap.release()\n",
        "  # count = count+1    "
      ],
      "metadata": {
        "id": "RfKNfwFQ3NRo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "17f6ebea-2748-42b7-913d-ccbf7d78d771"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read a new frame:  True\n",
            "Read a new frame:  True\n",
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6871cd910521>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0;31m# resize image into input shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0;31m# to be implement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m           \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/content/data/rowImg/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m           \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Read a new frame: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import cv2 as cv\n",
        "\n",
        "# for video in os.listdir('data/rowVid'):\n",
        "#   if video.endswith('mp4') or video.endswith('avi'):\n",
        "#     cap = cv.VideoCapture('/content/data/rowVid/'+video)\n",
        "#     c=1\n",
        "#     success,image = cap.read()\n",
        "#     while success:\n",
        "#       if c%100==0:\n",
        "#         cv.imwrite(f'/content/data/rowImg/'+ str(c) + '-' + video[:-4] + '.png', image)     # save frame as JPEG file      \n",
        "#         success,image = cap.read()\n",
        "#         print('Read a new frame: ', success)\n",
        "      #  x\n",
        "#       c= c+1\n",
        "   \n",
        "#     cap.release()"
      ],
      "metadata": {
        "id": "EnHfXIKJ2_1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load classifier model"
      ],
      "metadata": {
        "id": "R6AVigsS3D_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUAuiBAtQ2iW",
        "outputId": "e74b779d-af25-4b29-9bb7-0e03078cef4a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘model’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O model/emotionModel.hdf5 https://github.com/Furkan-Gulsen/face-classification/raw/main/models/emotionModel.hdf5\n",
        "# https://github.com/Furkan-Gulsen/face-classification/blob/main/emotionRecognition.py implementatino code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RLGraVDXYpj",
        "outputId": "004e9eab-4409-4733-ae23-018d1907f5aa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-08 09:04:18--  https://github.com/Furkan-Gulsen/face-classification/raw/main/models/emotionModel.hdf5\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Furkan-Gulsen/face-classification/main/models/emotionModel.hdf5 [following]\n",
            "--2023-05-08 09:04:18--  https://raw.githubusercontent.com/Furkan-Gulsen/face-classification/main/models/emotionModel.hdf5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 872856 (852K) [application/octet-stream]\n",
            "Saving to: ‘model/emotionModel.hdf5’\n",
            "\n",
            "model/emotionModel. 100%[===================>] 852.40K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-05-08 09:04:18 (18.4 MB/s) - ‘model/emotionModel.hdf5’ saved [872856/872856]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "metadata": {
        "id": "-Dd490V1X9pf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotionModelPath = 'model/emotionModel.hdf5'  # fer2013_mini_XCEPTION.110-0.65\n",
        "emotionClassifier = load_model(emotionModelPath, compile=False)\n",
        "emotionTargetSize = emotionClassifier.input_shape[1:3]"
      ],
      "metadata": {
        "id": "ULxsYExZSOKn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frame_to_prid(x,y,h,w,img):\n",
        "        grayFrame = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        grayFace = grayFrame[y:y + h, x:x + w]\n",
        "        \n",
        "        grayFace = cv2.resize(grayFace, (emotionTargetSize))\n",
        "    \n",
        "        grayFace = grayFace.astype('float32')\n",
        "        grayFace = grayFace / 255.0\n",
        "        grayFace = (grayFace - 0.5) * 2.0\n",
        "        grayFace = np.expand_dims(grayFace, 0)\n",
        "        grayFace = np.expand_dims(grayFace, -1)\n",
        "        emotion_prediction = emotionClassifier.predict(grayFace)\n",
        "        emotion_probability = np.max(emotion_prediction)\n",
        "        # print(emotion_prediction)\n",
        "        # print(emotion_probability)\n",
        "        emotion_label_arg = np.argmax(emotion_prediction)\n",
        "        print(emotions[emotion_label_arg]['emotion'])\n",
        "        print(emotion_label_arg)\n",
        "        return str(emotion_label_arg)"
      ],
      "metadata": {
        "id": "_Q2bTHctSV01"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = {\n",
        "    0: {\n",
        "        \"emotion\": \"Angry\",\n",
        "        \"color\": (193, 69, 42)\n",
        "    },\n",
        "    1: {\n",
        "        \"emotion\": \"Disgust\",\n",
        "        \"color\": (164, 175, 49)\n",
        "    },\n",
        "    2: {\n",
        "        \"emotion\": \"Fear\",\n",
        "        \"color\": (40, 52, 155)\n",
        "    },\n",
        "    3: {\n",
        "        \"emotion\": \"Happy\",\n",
        "        \"color\": (23, 164, 28)\n",
        "    },\n",
        "    4: {\n",
        "        \"emotion\": \"Sad\",\n",
        "        \"color\": (164, 93, 23)\n",
        "    },\n",
        "    5: {\n",
        "        \"emotion\": \"Suprise\",\n",
        "        \"color\": (218, 229, 97)\n",
        "    },\n",
        "    6: {\n",
        "        \"emotion\": \"Neutral\",\n",
        "        \"color\": (108, 72, 200)\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "hetezj_1Tp3P"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KEZyxpIsUy-y"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Faces from rowImage using mtcnn"
      ],
      "metadata": {
        "id": "tS3pX3_IIYlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybboxes mtcnn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EM_l6PxqVr-s",
        "outputId": "cef71862-205d-4052-b1f5-109870bdf2de"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pybboxes in /usr/local/lib/python3.10/dist-packages (0.1.6)\n",
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pybboxes) (1.22.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (4.7.0.72)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (2.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mtcnn import MTCNN\n",
        "import cv2\n",
        "from pybboxes import BoundingBox\n",
        "import os"
      ],
      "metadata": {
        "id": "dTEVfvb4IfVU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fn in os.listdir('/content/data/rowImg/'):\n",
        "  if os.path.exists(f'/content/data/rowImg/'+fn) and (fn.endswith('.png') or fn.endswith('.jpg')):\n",
        "    print(f'/content/data/rowImg/'+fn)\n",
        "    img = cv2.cvtColor(cv2.imread(f'/content/data/rowImg/'+fn), cv2.COLOR_BGR2RGB)\n",
        "    detector = MTCNN()\n",
        "    # print(detector.detect_faces(img))\n",
        "    for face in detector.detect_faces(img):\n",
        "      try:\n",
        "        # create bbox\n",
        "        a = face['box']\n",
        "        coco_bbox = BoundingBox.from_coco(*a)\n",
        "        coco_bbox.image_size = (640, 480)\n",
        "        yolo_bbox = coco_bbox.to_yolo()\n",
        "\n",
        "        #get prid\n",
        "        prid = frame_to_prid(a[0], a[1], a[2], a[3], img) \n",
        "\n",
        "        # save yolo bbox\n",
        "        f = open(f'/content/data/rowImg/'+fn[:-4]+'.txt', \"w\")\n",
        "        f.write(f''+ prid +' '+ str(yolo_bbox.values[0]) + ' '+ str(yolo_bbox.values[1]) + ' '+ str(yolo_bbox.values[2]) + ' '+ str(yolo_bbox.values[3]) + ' ')\n",
        "        f.close()\n",
        "      except:\n",
        "        continue\n",
        "      # save croped faces to new folder new file\n",
        "      "
      ],
      "metadata": {
        "id": "PyZ0q-rLK3ID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5e62f8-1d04-4ffe-d61e-1d358949fafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data/rowImg/262800-carpoolKaraoke.png\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Sad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link text](https://)## Extract frames(images from video)"
      ],
      "metadata": {
        "id": "5oHVEzlUiMKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/data/rowImg/14400-carpoolKaraoke.txt"
      ],
      "metadata": {
        "id": "rOqMijvA3BDe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
